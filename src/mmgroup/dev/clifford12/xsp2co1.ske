// %%COMMENT
// For this module we refer to the section
// 'Computation in the Clifford group' in the guide, see
// https://mmgroup.readthedocs.io/en/latest/





/*************************************************************************
** External references 
*************************************************************************/

#include <string.h>
#include "mat24_functions.h"

#define CLIFFORD12_INTERN
#include "clifford12.h"


// %%EXPORT_KWD CLIFFORD12_API


// %%GEN ch
#ifdef __cplusplus
extern "C" {
#endif
// %%GEN c


/*************************************************************************
*** Basic definitions
*************************************************************************/


// The standard short Leech lattice vector modulo 3
#define STD_V3  0x8000004ULL


// If ERROR_POOL is defined then function xp2co1_error_pool() can
// read data from an "error pool" that contains debug information
// for certain functions after calling them.
// #define ERROR_POOL

// Number of entries of type uit64_t of the ERROR_POOL
#define LEN_ERROR_POOL 20

// Exchange the bits masked by ``mask`` of the integer ``a``
// with the corresponding bits masked by ``mask << sh``.
// ``mask & (mask << sh)` = 0`` must hold. ``aux`` must be an 
// integer variable of the same type as variable ``a``.
#define SHIFT_MASKED(a, aux, mask, sh) \
    aux = (a ^ (a >> sh)) & mask; \
    a ^=  aux ^  (aux << sh);


// Standard size of a buffer for a quaratic state matrix
// representing an element of the group G_{{x0}}. 
#define MAXROWS_ELEM 30


/*************************************************************************
*** Using a pool for recording errors (for debugging)
*************************************************************************/



#ifdef  ERROR_POOL

static uint64_t error_pool[LEN_ERROR_POOL];
#endif

// %%EXPORT px
uint32_t xp2co1_error_pool(uint64_t *dest, uint32_t length)
{
  #ifdef ERROR_POOL
    uint32_t i;
    if (length > LEN_ERROR_POOL) length = LEN_ERROR_POOL;
    for (i = 0; i < length; ++i) dest[i] = error_pool[i];   
    return length;
  #else
    return 0;  // Dummy if  ERROR_POOL is not #defined.
  #endif   // #ifdef  ERROR_POOL 
}



/*************************************************************************
*** Functions for supporting the Leech lattice mod 3
*************************************************************************/


// %%EXPORT px
uint64_t xp2co1_short_2to3(uint64_t x)
{
    uint64_t  gcodev, cocodev, theta, w, result;
    // Put gcodev = codeword (in vector rep)
    gcodev = mat24_gcode_to_vect((uint32_t)x >> 12); 
    theta = MAT24_THETA_TABLE[((uint32_t)x >> 12) & 0x7ff]; 
    // Put w = weight(code word gcodev) / 4
    w = 0 - ((x >> 23) & 1);
    w = (((theta >> 12) & 7) ^ w) + (w & 7);  
    #ifdef ERROR_POOL
        memset(error_pool, 0, sizeof(error_pool));
        error_pool[0] = x; error_pool[1] = gcodev; 
        error_pool[2] = theta; error_pool[3] = w; 
    #endif    

    if (x & 0x800) {  // case odd cocode
        uint_fast32_t scalar; 
        // Put cocodev = cocode word (in vector rep)
        cocodev = mat24_cocode_syndrome((uint32_t)(x ^ theta), 0);    
        #ifdef ERROR_POOL
            error_pool[4] = cocodev; 
        #endif    
        if (cocodev & (cocodev - 1)) return 0;
        // Put scalar = scalar product (code, cocode)
        scalar = (x >> 12) &  x & 0xfff;
        scalar ^= scalar >> 6;
        scalar ^= scalar >> 3;
        scalar = (0x96 >> (scalar & 7));
        #ifdef ERROR_POOL
            error_pool[5] = scalar & 1; 
        #endif    
        if (scalar & 1) return 0;
        result = (gcodev ^ ((gcodev ^ 0xffffff) << 24))
               & ~(cocodev | (cocodev << 24));
        #ifdef ERROR_POOL
            error_pool[6] = result; 
        #endif 
        return result;        
    } else { 
        uint_fast32_t  c_w;
        uint8_t c_list[4];
        // Put x[11...0] = cocode word (in cocode rep)
        x ^= theta; 
        #ifdef ERROR_POOL
            error_pool[4] = x; 
        #endif    
        switch (w) {
            case 4:
                gcodev ^= 0xffffff;
            case 2:
                // Put cocodev = cocode word (in vector rep)
                cocodev = mat24_cocode_syndrome((uint32_t)x, 
                    mat24_lsbit24((uint32_t)gcodev));
                // Put c_w = min weight of cocode word
                c_w = mat24_bw24((uint32_t)cocodev);
                #ifdef ERROR_POOL
                    error_pool[4] = gcodev; 
                    error_pool[5] = cocodev; 
                    error_pool[6] = c_w; 
                #endif    
                if ( ((cocodev & gcodev) != cocodev)
                    ||  (c_w ^ 2 ^ w) & 3 ) return 0;
                result = (gcodev & ~cocodev) | (cocodev << 24);
                #ifdef ERROR_POOL
                    error_pool[7] = result; 
                #endif    
                return result;
            case 3:
                return 0;
            default:  // can be case 0 or 6 only
                // Put c_w = min weight of cocode word
                // and store cocode bits in c_list.
                c_w = mat24_cocode_to_bit_list((uint32_t)x, 0, c_list);
                #ifdef ERROR_POOL
                    error_pool[4] = c_w; 
                    error_pool[5] = c_list[0]; 
                    error_pool[6] = c_list[1]; 
                #endif    
                if (c_w != 2) return 0;
                result = (ONE <<  c_list[0]) + (ONE <<  (c_list[1] + 24 - 4 * w));
                #ifdef ERROR_POOL
                    error_pool[7] = result; 
                #endif    
                return result;
        }
    } 
}



static inline uint32_t short_3_scalprod(uint64_t x1, uint64_t x2)
{
    uint64_t zero, res;
    
    // Set all bits i in ``zero`` to 0 where x1[i] * x2[i] is 0
    zero = ((x1 ^ (x1 >> 24)) & (x2 ^ (x2 >> 24))) & 0xffffffUL;
    // Store scalar products of entries of x1 and x2 in res
    // Each scalar product is >= 0 and <= 2.
    res = (x1 ^ x2) & 0xffffff000000ULL;
    res = (res & (zero << 24)) | (zero & ~(res >> 24));
    // Sum up entries of res, counting the high 24 bits twice
    res = (res & 0x555555555555ULL) + ((res >> 1) & 0x555555555555ULL);
    res = (res & 0x333333333333ULL) + ((res >> 2) & 0x333333333333ULL);
    res = (res & 0x0f0f0f0f0f0fULL) + ((res >> 4) & 0x0f0f0f0f0f0fULL);
    res = (res & 0xffffffULL) + ((res >> 23) & 0x1fffffeULL);
    res = ((res >> 16) + (res >> 8) + res) & 0xff;
    // Reduce res modulo 3; we have 0 <= res <= 48
    res = (res & 3) + (res >> 2) + 1; // res <= 19; res is one too big
    res =  (res & 3) + (res >> 2);    // 1 <= res <= 7
    res =  (res & 3) + (res >> 2);    // 1 <= res <= 4
    res =  (res & 3) + (res >> 2);    // 1 <= res <= 3
    return (uint32_t)res - 1;
}


static inline uint64_t short_3_reduce(uint64_t x)
{
    uint64_t a = (x & (x >> 24)) & 0xffffffUL;
    x ^=  a | (a << 24);
    return x  & 0xffffffffffffULL;
}



// %%EXPORT px
uint64_t xp2co1_short_3to2(uint64_t x)
{
    uint_fast32_t  gcodev, cocodev, theta, w1, w2;
    x = short_3_reduce(x);
    w1 = mat24_bw24((uint32_t)x); 
    w2 = mat24_bw24((uint32_t)(x >> 24));
    switch (w1 + w2) {
        case 23:
            cocodev = ~(uint32_t)(x | (x >> 24)) & 0xffffffUL;
            if ((cocodev == 0) || (cocodev & (cocodev - 1))) return 0; 
            gcodev = (uint32_t)(x >> ((0-(w1 & 1)) & 24)) & 0xffffffUL;
            if ((w1 + 1) & 4)  gcodev ^= 0xffffffUL;
            break;              
        case 8:
            if (w1 & 1) return 0;
            gcodev = (x | (x >> 24)) & 0xffffffUL;
            cocodev = x & 0xffffffUL;
            if (w1 & 2) gcodev ^= 0xffffffUL;
            break;
        case 2:
            cocodev = (x |  (x >> 24)) & 0xffffffUL;
            gcodev = (w1 & 1) ? 0 : 0xffffffUL;
            break;
        default:
            return 0;        
    }
    gcodev = mat24_vect_to_gcode(gcodev);
    if (gcodev & 0xfffff000UL) return 0;
    theta = MAT24_THETA_TABLE[gcodev & 0x7ff] & 0xfff;
    cocodev = mat24_vect_to_cocode(cocodev);
    return (gcodev << 12) ^ theta ^ cocodev;
}



/*************************************************************************
*** Conversion between the two standard bases for group elements
*************************************************************************/


static inline
int32_t xp2co1_basis_to_conjugate(qstate12_type *pqs)
{
    return qstate12_gate_h(pqs, 0x800800);
}



static inline
int32_t xp2co1_conjugate_to_basis(qstate12_type *pqs)
{
    return qstate12_gate_h(pqs, 0x800800);
}




/*************************************************************************
*** Chains of short vectors in the Leech lattice mod 3
*************************************************************************/


// %%EXPORT px
uint64_t xp2co1_find_chain_short_3(uint64_t x1, uint64_t x2)
{
    uint64_t mask;
    uint_fast32_t support1, support2, c1, c2;
    x1 = short_3_reduce(x1);
    x2 = short_3_reduce(x2);
    // Compute the support of x1 in variable support1
    support1 = (uint32_t)((x1 | (x1 >> 24)) & 0xffffffUL);
    // Compute the support of x2 in variable support2
    support2 = (uint32_t)((x2 | (x2 >> 24)) & 0xffffffUL);
    if (support1 & ~support2) {
        // Find a bit c1 in (support1 & ~support2) and a bit c2 in
        // support2 and return a vector with support at bit positions
        // at c1 and c2, and entries taken from vector x1.
        // Fail if x2 ==  0
        c1 = mat24_lsbit24(support1 & ~support2);
        c2 = mat24_lsbit24(support2);
        if (c2 >= 24) return 0;
        mask = (ONE << c1) ^ (ONE << c2);
        mask = x1 & (mask | (mask << 24));
        if ((mask & (mask-1)) == 0) mask |= (ONE << c2);
        return mask;
    }
    if (support2 & ~support1) {
        // Similar to previous case, exchanging the roles of x1 and x2.
        c2 = mat24_lsbit24(support2 & ~support1);
        c1 = mat24_lsbit24(support1);
        if (c1 >= 24) return 0;
        mask = (ONE << c1) ^ (ONE << c2);
        mask = x2 & (mask | (mask << 24));
        if ((mask & (mask-1)) == 0) mask |= (ONE << c1);
        return mask;
    }
    if (support2 & support2) {
        // If (~support1 & ~support2) is not empty then take an entry
        // with one nonzero value at a position in (support2 & support2)
        // and one nonzero value at a position in (~support2 & ~support2).
        c1 = mat24_lsbit24(support1 & support2);
        c2 = mat24_lsbit24(~support1 & ~support2);
        if (c2 < 24) return (ONE << c1) ^ (ONE << c2);
        // Here the support of both, x1 and x2, comprises all 24 bits.
        // Find two bit positions c1 and c2, where both, x1 and x2, have 
        // equal values. Return a vector with support at bit positions
        // at c1 and c2, and entries taken from vector x1.
        mask = (x1 ^ x2) & 0xffffffUL;
        if ((mask & (mask-1)) == 0) mask ^= 0xfffffffUL;
        c1 = mat24_lsbit24((uint32_t)mask);
        mask ^= ONE << c1;
        c2 = mat24_lsbit24((uint32_t)mask); 
        mask = (ONE << c1) ^ (ONE << c2);       
        return x1 & (mask | (mask << 24));
    }
    return 0;
}





// %%EXPORT p
int32_t xp2co1_chain_short_3(qstate12_type *pqs, uint32_t length, uint64_t *psrc, uint64_t *pdest)
{
    qstate12_type qs;
    uint64_t qs_data[MAXROWS_ELEM], dest0, prod, src_prod;
    int_fast32_t res, ok;
    uint_fast32_t i;

    if bad_state(pqs) return ERR_QSTATE12_INCONSISTENT;
    if (pqs->ncols != 24 || pqs->shape1 != 12) 
         return ERR_QSTATE12_SHAPE_OP;
    
    // Create temporary copy of *pqs
    res = qstate12_copy_alloc(pqs, &qs, qs_data, MAXROWS_ELEM);
    if (res < 0) return res;
    res = xp2co1_basis_to_conjugate(&qs);
    if (res < 0) return res;    
    if (length <= 0) return 0;
    dest0 = short_3_reduce(pdest[0]);
    for (i = 0; i < length; ++i) pdest[i] = xp2co1_short_3to2(psrc[i]);
    res = qstate12_pauli_conjugate(&qs, length, pdest);
    if (res < 0) return res;
    for (i = 0; i < length; ++i) pdest[i] = xp2co1_short_2to3(pdest[i]);
    ok = (pdest[0] ==  dest0) || 
        (short_3_reduce(pdest[0] ^ 0xffffffffffffULL) == dest0);
    pdest[0] = ok ? dest0 : 0;
    for (i = 1; i < length;  ++i) {
        src_prod = short_3_scalprod(psrc[i-1], psrc[i]);
        prod =  short_3_scalprod(pdest[i-1], pdest[i]);
        if (prod != src_prod) pdest[i] = 
            short_3_reduce(pdest[i] ^ 0xffffffffffffULL);
        ok = ok && src_prod  &&  prod;
    }
    return ok ? 0 : -13;
}



/*************************************************************************
*** Conversion between quadratic state matrices and group elements
*************************************************************************/


// %%EXPORT p
int32_t xp2co1_elem_to_qs(uint64_t *e, qstate12_type *pqs)
{
    int32_t res;
    res = qstate12_set_mem(pqs, e + 1, 25); 
    if (res < 0) return res;
    pqs->maxrows = pqs->nrows = 25;
    pqs->ncols = 24;
    pqs->shape1 = 12;
    pqs->factor = -12 * 16;
    while (pqs->nrows > 1 && pqs->data[pqs->nrows - 1] == 0) {
        --pqs->nrows;
        pqs->factor += 16;
    }
    return qstate12_check(pqs);
}



static inline 
int32_t xp2co1_qs_to_elem_noreduce(qstate12_type *pqs, uint64_t x1, uint64_t *e)
{
    uint_fast32_t i;
    if (pqs->nrows > 25) return -5;
    for (i = 0; i < pqs->nrows; ++i) e[i+1] =  pqs->data[i];
    for (i = pqs->nrows; i < 25; ++i) e[i+1] = 0;
    if (pqs->factor & 4) x1 ^= 0xffffffffffffULL;
    e[0] = short_3_reduce(x1);  
    return 0;    
}

// %%EXPORT p
int32_t xp2co1_qs_to_elem(qstate12_type *pqs, uint64_t x1, uint64_t *e)
{
    int32_t res;
    res = qstate12_reduce(pqs); 
    if (res < 0) return res;
    res = qstate12_check(pqs);
    if (res < 0) return res;
    return  xp2co1_qs_to_elem_noreduce(pqs, x1, e);    
}



// %%EXPORT px
int32_t xp2co1_reduce_elem(uint64_t *e)
{
    int32_t res;
    qstate12_type qs;
    res = xp2co1_elem_to_qs(e, &qs);
    if (res < 0) return res;
    return xp2co1_qs_to_elem_noreduce(&qs, e[0], e); 
}


/*************************************************************************
*** Elementary function operating on group elements
*************************************************************************/




// %%EXPORT px
void xp2co1_neg_elem(uint64_t *e)
{
    e[0] = short_3_reduce(e[0] ^ 0xffffffffffffULL);
}


// %%EXPORT px
void xp2co1_copy_elem(uint64_t *e1, uint64_t *e2)
{
    uint_fast32_t i;
    for (i = 0; i < 26; ++i) e2[i] = e1[i];
}






/*************************************************************************
*** Group multiplication and inversion
*************************************************************************/


// %%EXPORT px
int32_t xp2co1_mul_elem(uint64_t *e1, uint64_t *e2, uint64_t *e3)
{
    int32_t res;
    qstate12_type qs1, qs2, qs3;
    uint64_t data3[MAXROWS_ELEM], asrc[3], adest[3];
    res = xp2co1_elem_to_qs(e1, &qs1);
    if (res < 0) return res;
    res = xp2co1_elem_to_qs(e2, &qs2);
    if (res < 0) return res;
    res = qstate12_set_mem(&qs3, data3, MAXROWS_ELEM);
    if (res < 0) return res;
    res = qstate12_matmul(&qs2, &qs1, &qs3);
    if (res < 0) return res;
    
    asrc[0] = STD_V3;
    adest[0] = e2[0];
    asrc[2] = e1[0];
    asrc[1] = xp2co1_find_chain_short_3(asrc[0], asrc[2]);
    res = xp2co1_chain_short_3(&qs2, 3, asrc, adest);
    #ifdef ERROR_POOL
        memcpy(error_pool+9, asrc, 3 * sizeof(uint64_t));
        memcpy(error_pool+12, adest, 3 * sizeof(uint64_t));
    #endif    
    if (res < 0) return res;
    if (adest[2] == 0) return -12;
    res = xp2co1_qs_to_elem_noreduce(&qs3, adest[2], e3);
    return res;
}


// %%EXPORT px
int32_t xp2co1_inv_elem(uint64_t *e1, uint64_t *e2)
{
    int32_t res;
    qstate12_type qs1, qs2;
    uint64_t data2[MAXROWS_ELEM], asrc[3], adest[3];
    res = xp2co1_elem_to_qs(e1, &qs1);
    if (res < 0) return res;
    res = qstate12_copy_alloc(&qs1, &qs2, data2, MAXROWS_ELEM);
    if (res < 0) return res;
    res = qstate12_mat_inv(&qs2);
    if (res < 0) return res;

    asrc[0] = e1[0];
    adest[0] = STD_V3;
    asrc[2] = STD_V3;
    asrc[1] = xp2co1_find_chain_short_3(asrc[0], asrc[2]);
    res = xp2co1_chain_short_3(&qs2, 3, asrc, adest);
    if (res < 0) return res;
    if (adest[2] == 0) return -12;
    res = xp2co1_qs_to_elem_noreduce(&qs2, adest[2], e2);
    return res;
}


/*************************************************************************
*** Cnstruction of group elements
*************************************************************************/


// %%EXPORT px
void xp2co1_unit_elem(uint64_t *e)
{
    uint_fast32_t i;
    uint64_t mask = 0x800800ULL;
    e[0] = STD_V3;
    e[1] = 0;
    for (i = 2; i < 14; ++i) {
        e[i] = mask;
        mask >>= 1;
    }
    for (i = 14; i < 26; ++i) e[i] = 0;
}


static 
int32_t elem_delta_pi_aut(uint64_t *e1, uint8_t perm[24], uint32_t aut[12])
{
    int32_t res;
    uint_fast32_t i;
    uint64_t data[MAXROWS_ELEM], x;
    qstate12_type qs;
    
    data[0] = 0;
    for (i = 0; i < 12; ++i) data[i+1] = aut[i];
    res = qstate12_set_mem(&qs, data, MAXROWS_ELEM);
    if (res < 0) return res;
    res = qstate12_monomial_column_matrix(&qs, 12, data);
    if (res < 0) return res;
    res = xp2co1_conjugate_to_basis(&qs);
    if (res < 0) return res;
    
    x = (ONE << perm[2]) + (ONE << (perm[3] + 24));
    return xp2co1_qs_to_elem(&qs, x, e1);
}




// %%EXPORT px
int32_t xp2co1_elem_delta_pi(uint64_t *e1, uint32_t delta, uint32_t pi)
{
    uint8_t perm[24];
    uint32_t aut[12];
    
    mat24_m24num_to_perm(pi, perm);
    mat24_perm_to_autpl(delta, perm, aut);
    return elem_delta_pi_aut(e1, perm, aut);
}




// %%EXPORT px
int32_t xp2co1_elem_x_delta(uint64_t *e1, uint32_t x, uint32_t delta)
{

    int32_t res;
    uint64_t v, t, data[MAXROWS_ELEM];
    qstate12_type qs;

    v =  ((x & 0x1fffUL) << 12) + (delta & 0xfffUL);
    v ^=  MAT24_THETA_TABLE[x & 0x7ff] & 0xfff;
    t = (delta ^ x) & 0x800UL;
    v ^= (t << 12) ^ t;    
    
    t = v & (v >> 12);
    t ^= t >> 6; t ^= t >> 3; 
    t = (0x96 >> (t & 7)) & 1;
    v ^= t << 24;
    
    res = qstate12_set_mem(&qs, data, MAXROWS_ELEM);
    if (res < 0) return res;
    res = qstate12_pauli_matrix(&qs, 12, v);
    if (res < 0) return res;
    return xp2co1_qs_to_elem_noreduce(&qs, STD_V3, e1);
}



static uint32_t MAPY_STD_V3[4] = {
   0x8000004UL, 0x4000008UL, 0xC, 0xC000000UL
};

// %%EXPORT px
int32_t xp2co1_elem_y(uint64_t *e1, uint32_t y)
{

    int32_t res, i;
    uint64_t d, s, data[MAXROWS_ELEM], assoc, theta_y, theta_d, v;
    qstate12_type qs;

    theta_y = MAT24_THETA_TABLE[y & 0x7ff] & 0x7ff;
    data[0] = y & 0x17ff;
    for (i = 0; i < 11; ++i) {
        d = ONE << i;
        theta_d = MAT24_THETA_TABLE[d & 0x7ff];
        s =  theta_d & y;
        s ^= s >> 6; s ^= s >> 3; 
        s = (0x96 >> (s & 7)) & 1;
        assoc = MAT24_THETA_TABLE[(d ^ y) & 0x7ff] ^ theta_d ^ theta_y;
        data[i+1] = d + (s << 12) + ((assoc & 0x7ff) << 13);
    } 
    data[12] = data[0] + 0x800 + (theta_y << 13);       
    res = qstate12_set_mem(&qs, data, MAXROWS_ELEM);
    if (res < 0) return res;
    res = qstate12_monomial_column_matrix(&qs, 12, data);
    if (res < 0) return res;

    v = ((y >> 8) ^ (y >> 10) ^ (y >> 11)) & 1;
    v ^= (y >> 8) & 2;
    return xp2co1_qs_to_elem(&qs, (uint64_t)MAPY_STD_V3[v], e1);
}




// The following table has been generated by function display_py_xi()
// in the python module  mmgroup.dev.clifford12.xs1_co1. The output 
// of that function has been copied to this place manually.
// This causes less overhead than using the code generator here.

static uint64_t elem_xi[2][12] = {
{
// Entries [0, 1, 2, 3, 10, 11, 12, 13, 14, 15, 16, 17] of element xi**1
0x000004000008ULL, 0x000000000400ULL, 0x000000800400ULL, 0x000000400c00ULL,
0x01dc00008000ULL, 0x01ba00004000ULL, 0x017600002000ULL, 0x00ee00001000ULL,
0x001c00000008ULL, 0x001a00000004ULL, 0x001600000002ULL, 0x000e00000001ULL
},
{
// Entries [0, 1, 2, 3, 10, 11, 12, 13, 14, 15, 16, 17] of element xi**2
0x000004000008ULL, 0x000000000800ULL, 0x000000800c00ULL, 0x000000400800ULL,
0x01c000008000ULL, 0x01a000004000ULL, 0x016000002000ULL, 0x00e000001000ULL,
0x01dc00000008ULL, 0x01ba00000004ULL, 0x017600000002ULL, 0x00ee00000001ULL
}
};

// %%EXPORT px
void xp2co1_elem_xi(uint64_t *e, uint32_t exp)
{
    uint_fast32_t i;
    uint64_t *p_xi;
    xp2co1_unit_elem(e);
    exp %= 3;
    if (exp == 0) return;
    p_xi = elem_xi[exp - 1];
    for (i = 0; i < 4; ++i) e[i] = p_xi[i];
    for (i = 4; i < 12; ++i) e[i + 6] = p_xi[i];
}




/*************************************************************************
*** Converting a an element to a vector of the monster rep modulo 3
*************************************************************************/



static inline
uint64_t xp2co1_to_vect_mod3(uint64_t x)
{
    uint64_t y;
    x = short_3_reduce(x);
    x = (x & 0xffffffULL) + ((x & 0xffffff000000ULL) << 8);
    SHIFT_MASKED(x, y, 0x00000000FFFF0000ULL, 16);
    SHIFT_MASKED(x, y, 0x0000FF000000FF00ULL, 8);
    SHIFT_MASKED(x, y, 0x00F000F000F000F0ULL, 4);
    SHIFT_MASKED(x, y, 0x0C0C0C0C0C0C0C0CULL, 2);
    SHIFT_MASKED(x, y, 0x2222222222222222ULL, 1);
    return x;
}

// %%EXPORT px
int32_t xp2co1_elem_row_mod3(uint64_t *e1, uint32_t column, uint64_t *v)
{
    int32_t res, sign;
    qstate12_type qs1, qs2;
    uint64_t data2[MAXROWS_ELEM], x;
 
    uint64_t n_iterations, i;
    uint64_t assoc; 
    uint64_t *m, x_data[2];
    uint64_t qf = 0;
    uint64_t ncols, mask; 

    res = xp2co1_elem_to_qs(e1, &qs1);
    if (res < 0) return res;
    res = qstate12_copy_alloc(&qs1, &qs2, data2, MAXROWS_ELEM);
    if (res < 0) return res;
    res = qstate12_gate_not(&qs2, column & 0xfff);
    if (res < 0) return res;
    res = qstate12_restrict(&qs2, 0, 12);
    if (res < 0) return res;
    res = qstate12_reduce(&qs2);
    if (res < 0) return res;
    if (qs2.factor & 0x13) return -14;
    sign = ((qs2.factor >> 5) ^ (qs2.factor >> 2)) & 1;
    x = e1[0];
    if (sign) x ^= 0xffffffffffffULL;
    x_data[0] = xp2co1_to_vect_mod3(x);
    x_data[1] = xp2co1_to_vect_mod3(x ^ 0xffffffffffffULL);

    n_iterations = ONE << (qs2.nrows - 1);
    m = qs2.data;
    assoc = m[0]; 
    ncols = qs2.ncols; 
    mask = (ONE << ncols) - 1;
    for (i = 0; i < ONE << ncols; ++i) v[i] = 0;

    for (i = 1; i <= n_iterations; ++i) {
        uint64_t i1, d, d1, index;
        index = assoc & mask;
        v[index] = x_data[qf & 1];
        d1 = d = qstate12_lsbtab[(i1 = i) & 63];
        while (d1 == 6) {
            i1 >>= 6;
            d1 = qstate12_lsbtab[i1 & 63];
            d += d1;
        } 
        qf += assoc >> (ncols + d + 1);
        assoc ^= m[d+1];
    } 
    return 0;   
}




/*************************************************************************
*** Converting a short vector modulo 3 to a Leech lattice vector
*************************************************************************/




static
int32_t xp2co1_add_short_3_leech(uint64_t x, int32_t factor, int8_t *psrc, int8_t *pdest)
// Given a short Leech lattice vector ``x`` (modulo 3), and short 
// Leech latice vectors ``src`` and ``dest``, referred by ``psrc`` and 
// ``pdest``, the function computes  ``dest = src + factor * x``.
// Here ``src`` and ``dest`` are given in the standard basis, so that
// a unit vector of length ``sqrt(8)`` has one entry with absolute
// value ``8``.
{
    uint_fast32_t  gcodev, cocodev, w1, w2;
    int_fast8_t f[4], i;
    f[0] = f[3] = 0;
    x = short_3_reduce(x);
    w1 = mat24_bw24((uint32_t)x); 
    w2 = mat24_bw24((uint32_t)(x >> 24));
    switch (w1 + w2) {
        case 23:
            cocodev = ~(uint32_t)(x | (x >> 24)) & 0xffffffUL;
            if ((cocodev == 0) || (cocodev & (cocodev - 1))) return -13;
            f[0] = (int8_t)factor * ((w1 & 1) ? -3 : 3); 
            f[1] = (int8_t)factor;
            gcodev = (uint32_t)(x >> ((0-(w1 & 1)) & 24)) & 0xffffffUL;
            break;              
        case 8:
            if (w1 & 1) return -13;
            gcodev = (x | (x >> 24)) & 0xffffffUL;
            f[1] = -2 * (int8_t)factor;
            break;
        case 2:
            gcodev = 0;
            f[1] = 4 * (int8_t)factor;
            break;
        default:
            return -13;        
    }
    f[2] = -f[1];
    gcodev = mat24_vect_to_gcode(gcodev);
    if (gcodev & 0xfffff000UL) return -13;
    x = xp2co1_to_vect_mod3(x);

    for (i = 0; i < 24; ++i) {
        pdest[i] =  psrc[i] + f[(x >> (i << 1)) & 3];
    }
    return 0;
}



// %%EXPORT px
int32_t xp2co1_elem_to_leech_op(uint64_t *elem, int8_t *pdest)
// Given an element ``elem`` of the group ``G_{{x_1}}``, the 
// function computes a ``24 times 24`` matrix ``L`` in the
// array ``dest`` referred by ``pdest``. The entry ``8 * L[i,j]``
// is stored in ``dest[24*i+j]``. Matrix ``L`` is unique up to 
// sign. The sign is undocumented, but computed deteministically. 
// Function ``xp2co1_elem_to_qs(elem,...)``  computes a``
// ``4096 times 4096`` matrix ``M`` for the element ``elem``
// as an object of type ``qstate12_type``.  The Kronecker
// product of ``L`` and ``M`` represents the element ``elem``
// of the group ``G_{{x_1}}`` uniquely. Let ``M.T`` be the
// transposed matrix of ``M``. Matrices ``L`` and `M.T`` act 
// on the tensor product ``24_x (x) 4096_x``, which is a  
// representation of ``G_{{x_1}}``, by right multiplication.
{
    uint64_t src3[25], dest3[25];
    int_fast32_t res, i;
    qstate12_type qs;
    
    // We let ``elem`` act on a sequence of short vectors
    // ``4*e_{{i}} -  4*e_{{i+1}}``, with ``e_i`` the ``i-``th
    // unit vector of the Leech lattice, and indices 
    // ``i = 2,...,25 `` taken modulo ``24``. We append the
    // unit vector ``4*e_{{2}} +  4*e_{{3}}`` to that sequence.
    // Adjacent vectors have nonzero scalar product modulo ``3``,
    // so we may use function ``xp2co1_chain_short_3`` to compute
    // the images of the vectors in that sequence under the group
    // element ``elem``. Form these images we may compute the
    // images of the unit vectors ``8 * e_i``, which form the
    // matrix ``8 * L``.

    for (i = 0; i <= 20; ++i) src3[i] = STD_V3 << i;
    src3[21] =  0x1800000ULL;
    src3[22] =  STD_V3 >> 2;
    src3[23] =  STD_V3 >> 1;
    src3[24] =  0xc;
    dest3[0] =  elem[0];

    res =  xp2co1_elem_to_qs(elem, &qs);
    if (res < 0) return res;
    res = xp2co1_chain_short_3(&qs, 25, src3, dest3);
    if (res < 0) return res;

    memset(pdest + 2*24, 0, 24);
    res = xp2co1_add_short_3_leech(dest3[24], 1, pdest + 2*24, pdest + 2*24);
    memcpy(pdest + 3*24, pdest + 2*24, 24);
    res |= xp2co1_add_short_3_leech(dest3[0], 1, pdest + 2*24, pdest + 2*24);
    res |= xp2co1_add_short_3_leech(dest3[0], -1, pdest + 3*24, pdest + 3*24);
    res |= xp2co1_add_short_3_leech(dest3[23], 2, pdest + 2*24, pdest + 1*24);
    res |= xp2co1_add_short_3_leech(dest3[22], 2, pdest + 1*24, pdest + 0*24);
    res |= xp2co1_add_short_3_leech(dest3[21], 2, pdest + 0*24, 
              pdest + 23*24);
    for (i = 20; i >= 2; --i) {
       res |= xp2co1_add_short_3_leech(dest3[i], 2, pdest + (i+3)*24, 
           pdest + (i+2)*24);
    }
    return res;
}



// %%GEN ch
#ifdef __cplusplus
}
#endif




