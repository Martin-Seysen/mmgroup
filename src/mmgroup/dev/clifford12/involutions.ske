/** @file involutions.c
  File ``involutions.c`` contains  functions for transforming
  involutions of the  subgroup \f$G_{x0}\f$ 
  (of structure \f$2^{1+24}.\mbox{Co}_1\f$)  of the monster.

  We try to transform such involutions to a standard form 
  via conjugation by elements of the monster group. 
*/


/*************************************************************************
** External references 
*************************************************************************/

/// @cond DO_NOT_DOCUMENT 
#include <string.h>
#include "mat24_functions.h"
#define MMGROUP_GENERATORS_INTERN
#include "mmgroup_generators.h"
#define CLIFFORD12_INTERN
#include "clifford12.h"
/// @endcond  


// %%EXPORT_KWD CLIFFORD12_API


// %%GEN ch
#ifdef __cplusplus
extern "C" {
#endif
// %%GEN c


//  %%GEN h
/// @cond DO_NOT_DOCUMENT 
//  %%GEN c


/// @cond DO_NOT_DOCUMENT 


/**
  @brief Return 0 if the square of a 24 times 24 matrix is zero

  Here ``m`` is a 24 times 24 bit matrix. The function returns 0 
  if the square of ``m`` is zero and a nonzero value otherwise.

*/
static inline uint64_t square_mat24_nonzero(uint64_t *m)
{
    uint_fast32_t i, j; 
    uint64_t mi, mo, result;
    result = 0;
    for (i = 0; i < 24; ++i) {
         mi = m[i]; mo = 0;
         for (j = 0; j < 24; ++j) {
             mo ^= (0 - ((mi >> j) & ONE)) & m[j];
         }
         result |= mo;
    }
    return result & 0xffffff;
}



/**
  @brief Return type of Leech lattice vector mod2

  Here ``v`` is a vector in the Leech lattice mod 2. The function
  returns the type of ``v`` modulo 2.

*/
static inline uint64_t leech_type_mod2(uint64_t v)
{
    uint64_t x = v;
    x &= (x >> 12);
    x ^= x >> 6; 
    x ^= x >> 3;
    return (0x96ULL >> (x & 7)) & 1;
}

/// @endcond 




/*************************************************************************
*** Orthogonal complment in Leech lattice mod 2
*************************************************************************/


/**
   @brief Compute standard orthogonal complment in Leech lattice mod 2

   Let \f$A = a_0\ldots, a_{k-1}\f$ be a matrix of \f$k\f$ vectors
   in the Leech lattice mod 2 stored in the array ``a``. The function
   returns a basis  \f$B = b_0\ldots, b_{23}\f$ of the Leech lattice
   mod 2 in the array ``b``, and it returns a number \f$m\f$ such that
   the vectors \f$b_m\ldots, b_{23}\f$ are a basis of the orthogonal
   complement of the space generated by the row vectors of \f$A\f$.

   If the vectors  \f$(a_0\ldots, a_{k-1})\f$ are linear independent
   then the function returns \f$m = k\f$, and vector \f$b_i, i < k\f$
   is orthogonal to all vectors  \f$a_j\f$ with \f$j \neq i\f$.

   The basis \f$B = b_0\ldots, b_{23}\f$ is stored in the array ``b``.

   We require \f$k \leq 24\f$. The function returns \f$m \geq 0\f$
   in case of success anegative value in case of failure.
*/
// %%EXPORT px
int32_t xsp2co1_orthogonal_complement(uint64_t *a, uint64_t *b, uint32_t k)
{
   uint64_t x;
   uint32_t i, m;

   if (k > 24) return ERR_QSTATE12_PARAM;

   // We store a 24 times k matrix Bh in columns 24,...,24+k-1 of 
   // the array b and a 24 times 24 matrix Bl in columns 0,...,23
   // of the array b.

   // Put Bl = A^T (with A^T the transposed matrix of A)
   bitmatrix64_t(a, k, 24, b);

   // Let Q be the scalar product matrix for the Leech lattice mod 2.
   // Put Bh = Q * A^T, i.e. exchange row i of A^T with row i+12.
   for (i = 0; i < 12; ++i) {
       x = b[i]; b[i] = b[i+12] << 24; b[i+12] = x << 24;
   }

   // Store the unit matrix in Bl 
   for (i = 0; i < 24; ++i)  b[i] |= 1ULL << i;

   // Echelonize Bh. This corresponds to left multiplication with a
   // nonsingular matrix B. W also multiply Bl (containing the unit
   // matrix) with B. So we have  Bl = B, Bh = B * Q * A^T,  and Bh 
   // is echelonized, containing m nonzero rows  and 24 - m zero rows. 
   // Thus B is the result, with the orthogonal complement of A in 
   // rows m,...,23 of B.  
   //
   // Remark: If the rows of A are linear independent then the upper
   //         k rows of  B * Q * A^T  form a unit matrix.
   m = bitmatrix64_echelon_l(b, 24, 24, k);

   // Output B = Bl and return m
   for (i = 0; i < 24; ++i) b[i] &= 0xffffffUL;
   return m;
}


/*************************************************************************
*** Invariants of an invloution in G_x0 / Q_x0
*************************************************************************/



/**
   @brief Compute invariant spaces for an involution in \f$G_{x0}\f$

   TODO: Documentation yet to be improved!!!


   Let \f$g\f$ be the element of the group \f$G_{x0}\f$ stored in the
   array given by parameter ``elem``. Let \f$\Lambda_2\f$ be the
   Leech lattice mod 2, with vectors in \f$\Lambda_2\f$ coded
   in **Leech lattice encoding** as usual. Conjugation by \f$g\f$ is a
   linear operation on  \f$\Lambda_2\f$, since the vectors
   in \f$\Lambda_2\f$ correspond to the elements of the normal
   subgroup \f$Q_{x0}\f$ of structure \f$2^{1+24}\f$ (modulo the
   centre of \f$G_{x0}\f$). Let \f$A = A(g)\f$ be the \f$24 \times 24\f$
   bit matrix that performs this operation on \f$\Lambda_2\f$ by
   right multiplication. Put  \f$A_1 = A - 1\f$, and let
   \f$I_1\f$ be the iamge of matrix  \f$A_1\f$. 

   In this function we require that the image of \f$g\f$ in the
   factor group  \f$\mbox{Co}_1\f$ of  \f$G_{x0}\f$ has order
   1 or 2; othereise the function fails. That condition is
   equivalent to \f$A^2 = 1\f$, and also to \f$A_1^2 = 0\f$. If
   this is the case the we have:

   \f$(\ker A_1)^\perp = I_1  \subset \ker A_1 = (I_1)^\perp\f$. 

   Any element \f$v \in \ker A_1\f$ is invariant under \f$g\f$, and so
   the corresponding element in \f$Q_{x0}\f$ is invariant up to sign.
   The elements of \f$Q_{x0}\f$ invariant under  \f$g\f$ (modulo
   the center of \f$Q_{x0}\f$) form a subspace\f$(\ker A_1)^+\f$ 
   of \f$\ker A_1\f$ of codimension \f$0\f$ or \f$1\f$. 
   Let  \f$(I_1)^+\f$ be the orthogonal complement 
   of  \f$(\ker A_1)^+\f$. Then  \f$I_1\f$ has the 
   same codimension in  \f$(\ker A_1)^+\f$. The purpose of this
   function is to compute the smaller of the two 
   spaces  \f$(I_1)^+\f$ or \f$\ker A_1\f$.

   We use the follwing column bits of the output matrix
    
   23,...,0:   Basis vector \f$v_i\f$ of \f$I_1\f$ or \f$(I_1)^+\f$

   55,...,32:  Preimage (under \f$A_1\f$) of basis vector \f$v_i\f$,
               undefined if \f$v_i \notin I_1\f$
   
   28:         A nozero bit in row 0 indicates an error.
   

   In bits 24,...,26  of the output matrix we return the following 
   linear forms of the basis vectors:

   Case 1:  \f$I_1 = (I_1)^+\f$ or \f$I_1 = \ker A_1 \f$

   Then we return a basis of \f$I_1\f$.
   
   Bit 26:    0

   Bit 25:    type of basis vector (modulo 2).

   Bit 24:    sign of basis vector in \f$I_1\f$

   Case 2:  \f$I_1 \neq (I_1)^+\f$ and  \f$I_1 \neq \ker A_1 \f$

   Then we return a basis of \f$(I_1)^+\f$.

   Bit 26:    0 iff basis vector  is in \f$I_1\f$

   Bit 25:    0 

   Bit 24:    sign of basis vector if vector is in \f$I_1\f$
              type of basis vector (mod 2) otherwise


   Bits 26,...,0  are  echelonized, so that bit 26 may be set in
   row 0 only.

   TODO: document special echelonization of basis vector!!!
 
   Parameter ``invar`` must be an array of length 12. Zero lines are
   appeded to that array so that its length will be 12.

   The function returns the dmenstion of the returned basis, and a 
   negative value in case of error. The return value 
   ERR_QSTATE12_GX0_BAD_ELEM means that that the image of \f$g\f$ in 
   the \f$\mbox{Co}_1\f$ has order grater than f$2\f$.
*/
// %%EXPORT px
int32_t xsp2co1_involution_invariants(uint64_t *elem, uint64_t *invar)
{
    uint64_t data[40], *pa = data + 16;
    uint64_t t0, t1;
    uint_fast32_t i, n;
    int_fast32_t status;

    // Initialize output with zeros and an error bit in row 0
    invar[0] = 0x8000000;
    for (i = 1; i < 12; ++i) invar[i] = 0;

    // Let `pa` be the 24 times 24 unit matrix 1
    for (i = 0; i < 24; ++i) pa[i] = ONE << i;

    // Conjugate row vectors of unit matrix pa with element
    // and store the the matrix A of the conjugated
    // row vectors in pa.
    status = xsp2co1_xspecial_conjugate(elem, 24, pa, 0);
    if (status < 0) return status;

    // Next we store two matrices PAH, PAL in pa, with PAL
    // in the lower 32 columns and PAH in the upper 32 columns
    // Put PAH = 1, PAL = A_1, where A_1 = A - 1 
    for (i = 0; i < 24; ++i) {
         pa[i] &=  0xffffffULL;
         pa[i] ^= 0x100000001ULL  << i;
    }

    // The group element `elem` is an involution (modulo the group  
    // Co_1) iff we have A_1**2 == 0. Otherwise we abort with an error.
    if (square_mat24_nonzero(pa)) return ERR_QSTATE12_GX0_BAD_ELEM; 

    // Echelonize PAL. So we left multiply both, PAH and PAL with a 
    // nonsingular matrix T auch that T * A_1 is echelonized. Then
    // PAH will contain T and PAL will contain  T * A_1.
    n = bitmatrix64_echelon_h(pa, 24, 24, 24);
    // Now the upper n rows of PAL contain the image I_1.
    // The upper n rows of PAH contain preimages of the rows of PAL.
    // The lower 24 - n rows of PAH contain the kernel \ker A_1.
    // The lower 24 - n rows of PAL are zero.

    // Deal with the A_1 == 0, i.e. `elem` is in O_2(G_{x0}) or,
    // equivalently, `elem` is neutral modulo Co_1.
    if (n == 0) {
        invar[0]  = xsp2co1_xspecial_vector(elem) & 0xffffff;
        if (invar[0] == 0)  return 0;
        invar[0] |= (leech_type_mod2(invar[0]) << 24) | 0x4000000;
        return 1;
    }
      
    if (n == 8) {
        // Deal with a 2A involution in Co_1

        // Move \ker A_1 to PAL and compute signs for \ker A_1
        bitmatrix64_rot_bits(pa + 8, 16, 32, 64, 0);
        status = xsp2co1_xspecial_conjugate(elem, 16, pa + 8, 1);
        if (status < 0) return status;

        // Make sure that at most the first row of \ker A_1
        // has negative sign
        bitmatrix64_echelon_h(pa + 8, 16, 25, 1);

        // Copy the image I_1 to output rows t1,...,t1+7, with
        // t1 = 0 if all entries of  \ker A_1 have positive sign
        // and t1 = 1 otherwise.
        t1 = (pa[8] >> 24) & 1;
        for (i = 0; i < 8; ++i) invar[t1 + i] = pa[i];

        // Skip the following steps for n == 8 if t1 is 0.
        if (t1 == 0) goto final_echelonize;

        // Now the first row vector of the matrix PAL[8..23] 
        // representing \ker A_1 has negative sign and the 
        // other row vectors of that matrix have positive sign.


        // Comupute the orthogonal complement (I_1)^+  of the 
        // positive part (\ker A_1)^+ of \ker A. We also compute the
        // orthogonal complement of \ker A_1 (which is I_1) in
        // such a way that we can find an vector in (I_1)^+
        // that is not in I_1. 
        xsp2co1_orthogonal_complement(pa + 8, data, 16);
        // Now we have computed a basis V of the Leech lattice mod 2
        // (in the array ``data``) such that V[16],...V[23] spans
        // the orthogonal complement I_1 of \ker A_1. By
        // definition of function ``xsp2co1_orthogonal_complement`` 
        // the vector v0 = V[0] is orthognal to (\ker A_1)^+, 
        // but not to vector \ker A_1. 

        // Thus v0 is in  (I_1)^+ but not in I_1.
        // Copy the v0 to the output row 0.
        invar[0] = data[0] & 0xffffff;

        // Set bit 26 of output row 0 to 1.
        invar[0] |= 0x4000000;

        // Set bit 24 of output row 0 to t0, with t0 = type(v0) (mod 2).
        t0 = leech_type_mod2(invar[0]);
        invar[0] |= t0 << 24;

        // Set bit 24 of the other output rows i = 1,...8 to t[i] with
        // t[i] = t0 + type(v0 + x[i])   (mod 2) .
        // Here x[i] is the output vector in row i.
        for (i = 1; i < 9; ++i) {
            t1 = t0 ^ leech_type_mod2(invar[0] ^ invar[i]);
            invar[i] |= t1 << 24; 
        }

        // Adjust number of output rows to 9.
        n = 9;
    } else if (n == 12) {
        // Deal with a 2B or 2C involution in Co_1

        // Copy the image the I_1 to the array ``data`` and 
        // store the signs of these images in bit 24.
        for (i = 0; i < 12; ++i) data[i] = pa[i];
        status = xsp2co1_xspecial_conjugate(elem, 16, data, 1);
        if (status < 0) return status;

        // Copy vectors pa[0,...,11] (containing I_1 and preimages
        // of these images) to output vector bits 23...0 and 55...32 
        // Copy signs of images to output vector bit 24
        // Copy type(output vector) modulo 2 to output vector bit 25.
        for (i = 0; i < 12; ++ i) {
             invar[i] = (pa[i] & 0xffffff00ffffffULL)
                     | (data[i] & 0x1000000)
                     | (leech_type_mod2(pa[i]) << 25);
        }
    } else {
        // Report failure
        return ERR_QSTATE12_REP_GX0;
    }


final_echelonize:
    // Echelonize the final result
    // Comment this later!!
    bitmatrix64_xch_bits(invar, n, 12, 0x800);
    bitmatrix64_rot_bits(invar, n, 1, 12,0);
    bitmatrix64_echelon_h(invar, n, 27, 27);
    bitmatrix64_rot_bits(invar, n, 11, 12,0);
    bitmatrix64_xch_bits(invar, n, 12, 0x800);

    // Zero preimage in row 0 if bit 26 in row 0 is set.
    invar[0] &= ((invar[0] & 0x4000000) << 2) - 1;
    return n;
    
}





/*************************************************************************
*** Application of invariants of an invloution in G_x0 / Q_x0
*************************************************************************/

/**
   @brief Compute some orthogonal complement for involution invariants

   TODO: document this!!!

   Let \f$g\f$ be the element of the group \f$G_{x0}\f$,
   and for that element \f$g\f$  
   let \f$A, A_1, I_1\f$, and \f$(I_1)^+\f$
   be as in function ``xsp2co1_involution_invariants``. 

   Here input parameter ``invar`` must be the output ``invar`` 
   of function ``xsp2co1_involution_invariants`` applied to the
   element \f$g\f$.

   We return a ``nice`` type-4 vector in \f$(I_1)^+\f$, if
   any type-4 vector exists in \f$(I_1)^+\f$; otherwise we
   return 0.

   Preliminary!! This is subject to change. 
*/
// %%EXPORT px
int32_t xsp2co1_involution_orthogonal(uint64_t *invar, uint32_t col)
{
   uint64_t M[12], T[24], v, *pA;
   int32_t n, i;

   // Select column `c` of input matrix a;
   // abort if ``col`` or ``invar`` is erroneous.
   if (col > 1) return ERR_QSTATE12_PARAM;
   if (invar[0] & 0x8000000) return ERR_QSTATE12_GX0_BAD_ELEM;
   col += 24;

   // Extract relevant rows of matrix a of invariants
   n = 12;
   while (n > 0 && invar[n-1] == 0) --n;
   pA = invar;
   if (pA[0] & 0x4000000) { 
       ++pA; --n;
   }
   if (n == 0) return 0;
   // Now the relevant rows of a are pA[j], 0 <= j < n;
   // Let A be the relevant part of the kernel in a
   // and P be the relevant part of the preimage of A
   // A is in columns 23,...,0 and P is in columns 55,...,32.

   // Store column `c` of input matrix a in bit vector v.
   // Return 0 if that column is zero
   v = 0;
   for (i = 0; i < n; ++i) v |= ((pA[i] >> col) & ONE) << i; 
   if (v == 0) return 0;

   // Put M = (A, P), with columns used as above.
   for (i = 0; i < n; ++i) M[i] = pA[i];
   bitmatrix64_rot_bits(M, n, 32, 64, 0);
   
   // Put T = P^T (P^T is the transposed of P)
   bitmatrix64_t(M, n, 24, T);

   // Put M = (?, A * Q), where Q is the scalar poduct in the Leech e
   // lattice  mod 2; i.e. exchange column i of A with column i + 12.
   bitmatrix64_rot_bits(M, n, 32, 64, 0); // M = (?, A)
   bitmatrix64_rot_bits(M, n, 12, 24, 0); // M = (?, A * Q)

   // Put M = A * Q * P^T
   bitmatrix64_mul(M, T, n, 24, M);

   // Put M = J, with J = ( A * Q * P^T) ** (-1);
   // abort if inverse dose not exist.
   if (bitmatrix64_inv(M, n) < 0) return ERR_QSTATE12_REP_GX0;

   // Put v = c * J
   bitmatrix64_mul(&v, M, 1, n, &v);

   // Put v = c * J * A
   bitmatrix64_mul(&v, pA, 1, 24, &v);
   v &= 0xffffffULL;
   
   // Now v is in the space spanned by A and we have
   // v * Q * P^T = c. So v is the result
   return (int32_t) v;
}                




/*************************************************************************
*** Find type-4 vector in involution invariants
*************************************************************************/
/// @cond DO_NOT_DOCUMENT 


/** @brief  List all vectors in an affine space.

    Let ``a``  = \f$a_0,...,a_{n-1}\f$ be a matrix of bit vectors and 
    let \f$a_a\f$  be the bit vector stored in ``aa``. Let 

    \f$b_m = a_{a} + \sum_{i = 0}^{k-1} \cdot m_i a_{n-1-i}\f$,

    where \f$m_{k-1},\ldots,m_0\f$ is the binary representation
    of \f$m\f$.
    
    We write \f$b_m\f$ to ``b[m]`` for \f$0 \leq m < 2^k\f$.
    We change \f$k\f$ to \f$\max(0, \min(24, n ,k))\f$ and 
    return the changed value \f$k\f$.   
*/
static inline uint32_t expand_affine(
    uint64_t *a, uint32_t n, int32_t k, uint64_t a0, uint32_t *b
)
{
    uint32_t exp, i, j, v;
    b[0] = (uint32_t)(a0);
    if (k <= 0) return 0;
    if ((uint32_t)k > n) k = n;
    if (k > 24) k = 24;
    exp = 1;
    for (i = 0; i < (uint32_t)k; ++i) {
        v = (uint32_t)(a[k-i-1]);
        for (j = 0; j < exp; ++j) b[exp + j] = v ^ b[j];
        exp <<= 1;
    }
    return k;              
}
/// @endcond 




/** @brief yet to be documented.

    Here ``a`` is a sequence of three arrays ``a0, a1,`` and ``a2``
    of elements of the Leech lattice mod 2
    of length ``n0, n1,`` and ``n2`` respectively. ``a0`` and ``a1,``
    are input arrays and ``a2`` is an output array.

    The function runs through all vectors ``a0[i0] + a1[i1]``
    with ``0 <= i0 < n0`` and ``0 <= i1 < n1``. Let ``V`` be
    the set of these vectors. All vectors in the set ``V``
    must be of Leech lattice type 0, 2, or 4; otherwise the function
    computes garbage.

    If ``n2`` is zero then the function returns a type-4 vector
    in ``V`` if such a vector exists; otherwise it returns zero.

    If ``n2`` is positive then the function stores up to
    ``n2`` vectors of type 2 in the array ``a2`` and pads that
    array with zero vectors. The it  returns 0.

    The function tries to find a **nice** type-4 vector, provided
    that the input has been echelonized properly (as e.g. in
    function ``xsp2co1_involution_invariants``) and expanded properly
    as e.g. in function ``xsp2co1_involution_find_type4``. 
*/
// %%EXPORT px
uint32_t xsp2co1_involution_sub_find_type(uint32_t *a, uint32_t n0, uint32_t n1, uint32_t n2)
{  
   uint32_t i0, i1, i2, a1, v, v_out = 0;
   uint32_t is_type4;  // This will be True if a vector is of type 4
   uint32_t find_type4 = n2 == 0;
   uint32_t *pv2 = a + n0 + n1;

   for (i2 = 0; i2 < n2; ++i2) pv2[i2] = 0;
   i2 = 0;
   for (i1 = 0; i1 < n1; ++i1) {
       a1 = a[n0 + i1];
       for (i0 = 0; i0 < n0; ++i0) {
           // let v be the word to be analyzed
           v = (a[i0] ^ a1) & 0xffffff;

           if (v & 0x800UL) {
                // Deal with odd cocode words
                uint_fast32_t theta, tab, gcode;
                // Return a pre-stored even vector if present 
                // and if n2 is zero.
                if (v_out && n2 == 0) break;
                // Otherwise we'll have to check the odd vector v
                gcode = (v >> 12);
                theta = MAT24_THETA_TABLE[gcode & 0x7ff] ^ v;
                tab = MAT24_SYNDROME_TABLE[theta & 0x7ff];
                // v has type 4 if tab is does not encode
                // a cocode word of length 1
                is_type4 = (tab & 0x3ff) < (24 << 5);
            } else if ((v & 0x7ff000L) == 0) {
                // Deal with Golay code word 0 and even cocode word
                uint_fast32_t basis0, tab, b0, b1;
                // First reject the zero word
                if (v == 0) continue;

                // Let basis0 = cocode vector, flip bit 0 in cocode
                basis0 = MAT24_RECIP_BASIS[0] ^ v;            
                // Let tab be the syndrome table entry for the cocode
                // part XORed with basis vector 0. 
                tab = MAT24_SYNDROME_TABLE[basis0 & 0x7ff];
                // Set b1 True iff tab encodes a cocode word of length 3
                b1 =  (tab & 0x3ff) < (24 << 5);
                // Set b0 True iff tab encodes a cocode word with bit 0 = 1
                b0 =  (tab & 0x1f) ==  0;
                //  b0 ^ b1 is True iff (the even) cocode word has length 4
                //  v has type 4 iff the cocode word has length 4
                is_type4 = b0 ^ b1; 
            } else if (mat24_def_not_nonstrict_octad(v >> 12) == 0) {
                uint_fast32_t vect, w0, lsb, theta, tab, syn, b0, b1;
                // Deal with code word that are octads
                // let vect = Golay code in vector representation
                vect = mat24_gcode_to_vect((v >> 12) & 0x7ff);
                // Put  w0 =  weight(code word) / 8     (mod 2);
                // so w0 is True iff the Golay code word is an octad.
                theta = MAT24_THETA_TABLE[(v >> 12) & 0x7ff];
                w0 = ((theta >> 13) ^ (v >> 23)) & 1;  
                // Complement vect if it is not an octad
                vect ^= w0 - 1;
                // Let lsb be the least significant bit of vect
                lsb = mat24_def_lsbit24(vect);
                // Adjust theta to be the odd cocode word correspodning
                // to ``v``  bit at position ``lsb`` set 
                // with the octad `vect` in `lsb` if it has length 4.
                theta ^= v ^ MAT24_RECIP_BASIS[lsb];
            
                // Let tab be the table entry for the cocode part XORed
                // with basis vector ``lsb`` flipped 
                tab = MAT24_SYNDROME_TABLE[(theta) & 0x7ff];
                // Compute cocode syndrome from tab
                syn = (1 << (tab & 31)) ^ (1 << ((tab >> 5) & 31))   
                                ^ (1 << ((tab >> 10) & 31));
                // v has type 4 if  syndrome is not a subword of ``vect``
                is_type4 = ((vect & syn) != syn);
                // Otherwise it might have type 2
                // Set b1 True iff tab encodes a cocode word of length 3
                b1 =  (tab & 0x3ff) < (24 << 5);
                // Set b0 True iff tab encodes a cocode word with 
                // bit ``lsb``  set
                b0 =  (tab & 0x1f) ==  lsb;
                // Put b0 = 1 if cocode word has length 0 or 4 
                // Put b0 = 0 if cocode word has length 2 
                b0 ^= b1;
                // ``v`` may be of type 2 if length/2 != w0 (mod 2)
                is_type4 |= b0 ^ w0;
            } else  {
                // Deal with docecad:
                // Since no dodecad is of type 2, we save the first dodecad
                // found and wait for something better to come.
                // We might put is_type4 = True here.
                if (v_out == 0) v_out = v;
                continue;
            } ;
            // Continue when searching for type 4 and found type 2 
            // or vice versa
            if (find_type4 != is_type4) continue;
            // Otherwise done if found a type-4 vector
            if (is_type4) return v;
            // .. and store a type-2 vector if found and yet 
            // enough space.
            if (i2 < n2) pv2[i2++] = v;
       }
   }
   // When searching for type 4 a dodecad has been found then
   // return that dodecad. Otherwise return zero.
   return find_type4 ? v_out : 0;
}





/**
   Find type-4 vector in a space computed by ``xsp2co1_involution_invariants``. 

   TODO: document this!!!

   Let \f$g\f$ be the element of the group \f$G_{x0}\f$,
   and for that element \f$g\f$ and
   let \f$A, A_1, I_1\f$, and \f$(I_1)^+\f$
   be as in function ``xsp2co1_involution_invariants``. 

   Here input parameter ``invar`` must be the output ``invar`` 
   of function ``xsp2co1_involution_invariants`` applied to the
   element \f$g\f$.

   In case ``coset = 0`` we return a **nice** type-4 vector 
   in \f$(I_1)^+\f$, if any type-4 vector exists in \f$(I_1)^+\f$; 
   otherwise we return 0.

   TODO: Document the case ``coset = 1``!!!

   Preliminary!! This is subject to change. 
*/
// %%EXPORT px
int32_t xsp2co1_involution_find_type4(uint64_t *invar, uint32_t coset)
{
   uint64_t *pInv;
   uint32_t n, n0, n1, a[49], v;

   coset = coset ? 0xffffff : 0; 

   // Abort if ``invar`` is erroneous.
   if (invar[0] & 0x8000000) return 0;

   // Delete tailing zero rows of matrix ``invar``
   n = 12;
   while (n > 0 && (invar[n-1] & 0xffffff) == 0) --n;

   // Delete leading zero rows of matrix ``invar`` which are not
   // in  \f$(I_1)^+\f$ or have odd scalar product with
   // the vector \f$\Omega\f$ in the Leech lattice.
   pInv = invar;
   while (n > 0 && (pInv[0] && 0xf000800) != 0) { 
       ++pInv; --n;
   }
   // Now the relevant space V is generated py pInv[i], 0 <= i < 8

   // Abort if V is has dimension 0, and delete leading rows 
   // of generating rows of V until V has dimension <= 8.
   if (n == 0) return 0;
   while (n > 8) { 
       ++pInv; --n;
   }
   
   n0 = expand_affine(pInv, n, 4, coset & invar[0], a);
   if (n - n0 > 4) return 0;
   n1 = expand_affine(pInv, n - n0, n - n0, 0, a);

   v = xsp2co1_involution_sub_find_type(a, 1 << n0, 1 << n1, coset & 16);  

   if (coset == 0) return v;

   a[48] = a[32];
   return xsp2co1_involution_sub_find_type(a, 16, 1,0); 
}



/** 
   TODO: Improve documentation


   A negative value indicates an error. Here
   ERR_QSTATE12_GX0_BAD_ELEM means that no further simplification
   is possble.

*/
// %%EXPORT px
int32_t xsp2co1_elem_find_type4(uint64_t *elem)
{
    uint64_t invar[12];   
    int_fast32_t v, t, n;

    if (xsp2co1_xspecial_vector(elem) >= 0) return 0x800000;

    n = xsp2co1_involution_invariants(elem, invar);
    if (n < 0) return n;
    switch (n) {
        case 8:
            return xsp2co1_involution_find_type4(invar, 0);
        case 9:
            v = xsp2co1_involution_orthogonal(invar, 0);
            t = gen_leech2_type(v) >> 4;
            if (t & 0xfffffffbUL) return ERR_QSTATE12_GX0_BAD_ELEM;
            if (t == 0) {
                 v = xsp2co1_involution_find_type4(invar, 1);
                 if (v == 0) return ERR_QSTATE12_GX0_BAD_ELEM;
            } 
            return v;
        case 12:
            v = xsp2co1_involution_orthogonal(invar, 1);
            t = gen_leech2_type(v) >> 4;
            if (t != 4) return  ERR_QSTATE12_GX0_BAD_ELEM; 
            return v;
        default:    
            return ERR_QSTATE12_GX0_BAD_ELEM;
    }
}

/********************************************
// Deprecated!!!!
int32_t xsp2co1_elem_find_type4_old(uint64_t *elem, int32_t last_action)
{
    uint64_t invar[12];   
    int_fast32_t v, t, n, i;
    uint32_t w[10], x, y, cocode;

    if ((v = xsp2co1_xspecial_vector(elem)) >= 0) {
        if (v == 0x800000) return 0x4000001;
        if (v == 0x1800000) return 0x4000002;
        if ((gen_leech2_type(v) >> 4) != 4) return v;
        return 0x2000000 | (v & 0xffffff);
    }

    if (last_action & 0x2000000) goto skip_type4;
    n = xsp2co1_involution_invariants(elem, invar);
    if (n < 0) return n;
    switch (n) {
        case 8:
            return xsp2co1_involution_find_type4(invar, 0);
        case 9:
            v = xsp2co1_involution_orthogonal(invar, 0);
            t = gen_leech2_type(v) >> 4;
            if (t & 0xfffffffbUL) return ERR_QSTATE12_GX0_BAD_ELEM;
            if (t == 0) {
                 v = xsp2co1_involution_find_type4(invar, 1);
                 if (v == 0) return ERR_QSTATE12_GX0_BAD_ELEM;
            } 
            break;
        case 12:
            v = xsp2co1_involution_orthogonal(invar, 1);
            t = gen_leech2_type(v) >> 4;
            if (t != 4) return  ERR_QSTATE12_GX0_BAD_ELEM; 
            break;
        default:    
            return ERR_QSTATE12_GX0_BAD_ELEM;
    }
    v &= 0xffffffUL;
    if (v != 0x800000) return 0x2000000 | v;
 
  skip_type4:
    x = y = cocode = 0;
    n = xsp2co1_elem_to_word(elem, w);
    if (n < 0) return n;
    for (i = 0; i < n; ++i) {
       switch ((w[i] >> 28) & 7) {
            case 0:  
                // ignore tag:
                break;
            case 1:
                cocode ^= w[i] & 0x800; 
                break;
            case 3:
                // tag x: accumulate in x
                x ^= w[i] & 0x7ff;
                break;
            case 4:
                // tag y: accumulate in y
                y ^= w[i] & 0x7ff;
                break;
            default:
                return ERR_QSTATE12_GX0_BAD_ELEM; 
        }
    }
    if (y == 0) return ERR_QSTATE12_REP_GX0;
    if (cocode & 0x800) return ERR_QSTATE12_GX0_BAD_ELEM;
    if (x == 0) return 0x40000001; 
    if (x == y) return 0x40000002;
    return ERR_QSTATE12_GX0_BAD_ELEM;
}
********************************************/

/*********************************
// deprecated
static inline int32_t transform_xsp(uint64_t *elem, uint32_t *a, uint32_t len_a)
{
     int32_t last = 0, len, res, i;
     uint32_t *pa = a;
     uint64_t elem1[26];
     
     for (i = 0; i < 4; ++i) {
         last = xsp2co1_elem_find_type4(elem, last);
         if (last < 0x2000000) return ((uint32_t)(pa -a) << 25) + last;
         if (last & 0x4000000) {
             if (len_a < 6) return ERR_QSTATE12_BUFFER_OVFL; 
             len =  gen_leech2_reduce_type4(last, pa);
             if (len < 0) return len;
         } else if (last & 0x4000000) {
             if (len < 1) return ERR_QSTATE12_BUFFER_OVFL; 
             pa[0] = MMGROUP_ATOM_TAG_T + (last & 3);
             len = 1;
         } else return ERR_QSTATE12_REP_GX0; 
         mm_group_invert_word(pa, len);
         res = xsp2co1_set_elem_word(elem1, pa, len);
         if (res < 0) return res;
         res = xsp2co1_mul_elem(elem1, elem, elem);
         if (res < 0) return res;
         mm_group_invert_word(a, len);
         res =  xsp2co1_mul_elem_word(elem, pa, len);
         if (res < 0) return res;
         pa +=len; len_a -= len;
     }  
     return ERR_QSTATE12_REP_GX0;   
}

**************************************************/

//  %%GEN h
/// @endcond 
//  %%GEN c


// %%GEN ch
#ifdef __cplusplus
}
#endif
