/** @file involutions.c
  File ``involutions.c`` contains  functions for transforming
  involutions of the  subgroup \f$G_{x0}\f$ 
  (of structure \f$2^{1+24}.\mbox{Co}_1\f$)  of the monster.

  We try to transform such involutions to a standard form 
  via conjugation by elements of the monster group. 
*/


/*************************************************************************
** External references 
*************************************************************************/

/// @cond DO_NOT_DOCUMENT 
#include <string.h>
#include "mat24_functions.h"
#define MMGROUP_GENERATORS_INTERN
#include "mmgroup_generators.h"
#define CLIFFORD12_INTERN
#include "clifford12.h"
/// @endcond  


// %%EXPORT_KWD CLIFFORD12_API


// %%GEN ch
#ifdef __cplusplus
extern "C" {
#endif
// %%GEN c


//  %%GEN h
/// @cond DO_NOT_DOCUMENT 
//  %%GEN c


/// @cond DO_NOT_DOCUMENT 


/**
  @brief Return 0 if the square of a 24 times 24 matrix is zero

  Here ``m`` is a 24 times 24 bit matrix. The function returns 0 
  if the square of ``m`` is zero and a nonzero value otherwise.

*/
static inline uint64_t square_mat24_nonzero(uint64_t *m)
{
    uint_fast32_t i, j; 
    uint64_t mi, mo, result;
    result = 0;
    for (i = 0; i < 24; ++i) {
         mi = m[i]; mo = 0;
         for (j = 0; j < 24; ++j) {
             mo ^= (0 - ((mi >> j) & ONE)) & m[j];
         }
         result |= mo;
    }
    return result & 0xffffff;
}



/**
  @brief Return type of Leech lattice vector mod2

  Here ``v`` is a vector in the Leech lattice mod 2. The function
  returns the type of ``v`` modulo 2.

*/
static inline uint64_t leech_type_mod2(uint64_t v)
{
    uint64_t x = v;
    x &= (x >> 12);
    x ^= x >> 6; 
    x ^= x >> 3;
    return (0x69ULL >> (x & 7)) & 1;
}

/// @endcond 


/*************************************************************************
*** Functions yet to be implemented!!!!
*************************************************************************/


/**
   @brief Compute some orthogonal complement in Leech lattice mod 2

   Preiminary!! This is subject to change. 

   Let \f$a = a_0\ldots, a_{k-1}\f$ and \f$b = b_0\ldots, b_{k-1}\f$
   be matrices of \f$k\f$ vectors in the Leech lattice mod2. Let
   \f$v = v_0\ldots, v_{k-1}\f$ be a bit vector, with \f$v_i\f$
   stored in bit \f$24\f$ of the integer \f$a_i\f$.

   Then we return an element \f$w\f$ in the space of vectors
   spanned by \f$(a_0\ldots, a_{k-1})\f$ such
   that \f$\langle w, b_i\right \rangle = v_i\f$ holds.
   Here \f$\langle .,. \rangle\f$ is the scalar product in
   the Leech lattice mod 2.

   The solution \f$w\f$ is unique if the \f$k \times k\f$ matrix
   of the scalar products \f$\langle a_i .,. b_j\rangle\f$ is
   invertible; otherwise the function fails. We also
   require \f$k <= 24\f$.

   The function returns ``(uint32_t)(-1)`` in case of failure.
*/
// %%EXPORT p
uint32_t xsp2co1_solve_scalar_prod(uint64_t *a, uint64_t *b, uint32_t k)
{
   uint64_t a1[24], b1[24], x, w, i;

   if (k > 24) return (uint32_t)(-1);
   
   // Put b1 = b^T, where b^T is the tranposed matrix of b
   for (i = 0; i < k; ++i) a1[i] = b[i]; 
   bitmatrix64_t(a1, k, 24, b1);

   // Exchange row i of b1 with from i+12. So b1 = Q * b^T, where 
   // Q is the scalar product matrix for the Leech lattice mod 2.
   for (i = 0; i < 12; ++i) {
      x = b1[i]; b1[i] = b1[i+12]; b1[i+12] = x;
   }

   // Put a1 = a.
   for (i = 0; i < k; ++i) a1[i] = a[i]; 

   // Put a1 = C, with C = a * Q * b^T.
   bitmatrix64_mul(a1, b1, k, 24, a1);

   // Put a1 = C**(-1); abort if matrix inversion fails.
   if (bitmatrix64_inv(a1, k) < 0) return (uint32_t)(-1);

   // Put w = v.
   w = 0;
   for (i = 0; i < 24; ++i) w += ((a[i] >> 24) & 1) << i;

   // Put w = v * C**(-1).
   bitmatrix64_mul(&w, a1, 1, k, &w);
   
   // Put w = v * C**(-1) * a.
   for (i = 0; i < k; ++i) a1[i] = a[i]; // Put a1 = a
   bitmatrix64_mul(&w, a1, 1, 24, &w);

   // Return w 
   return (uint32_t) w;
}                

/**
   @brief Compute standard orthogonal complment in Leech lattice mod 2

   Let \f$A = a_0\ldots, a_{k-1}\f$ be a matrix of \f$k\f$ vectors
   in the Leech lattice mod 2 stored in the array ``a``. The function
   returns a basis  \f$B = b_0\ldots, b_{23}\f$ of the Leech lattice
   mod 2 in the array ``b``, and it returns a number \f$m\f$ such that
   the vectors \f$b_m\ldots, b_{23}\f$ are a basis of the orthogonal
   complement of the space generated by the row vectors of \f$A\f$.

   If the vectors  \f$(a_0\ldots, a_{k-1})\f$ are linear independent
   then the function returns \f$m = k\f$, and vector \f$b_i, i < k\f$
   is orthogonal to all vectors  \f$a_j\f$ with \f$j \neq i\f$.

   The basis \f$B = b_0\ldots, b_{23}\f$ is stored in the array ``b``.

   We require \f$k \leq 24\f$. The function returns \f$m \geq 0\f$
   in case of success and  -1 in case of failure.
*/
// %%EXPORT p
int32_t xsp2co1_orthogonal_complement(uint64_t *a, uint64_t *b, uint32_t k)
{
   uint64_t x;
   uint32_t i, j, m;

   if (k > 24) return -1;

   // We store a 24 times k matrix Bh in columns 24,...,24+k-1 of 
   // the array b and a 24 times 24 matrix Bl in columns 0,...,23
   // of the array b.

   // Put Bh = A^T (with A^T the transposed matrix of A)
   for (i = 0; i < 24; ++i) {
       x = 0;
       for (j = 0; j < k; ++j) x |= ((a[j] >> i) & 1) << j;  
       b[i] = x << 24;    
   }

   // Let Q be the scalar product matrix for the Leech lattice mod 2.
   // Put Bh = Q * A^T, i.e. exchange row i of Bh with row i+12.
   for (i = 0; i < 12; ++i) {
       x = b[i]; b[i] = b[i+12]; b[i+12] = x;
   }

   // Store the unit matrix in Bl 
   for (i = 0; i < 24; ++i)  b[i] |= 1ULL << i;

   // Echelonize Bh. This corresponds to left multiplication with a
   // nonsingular matrix B. W also multiply Bl (containing the unit
   // matrix) with B. So we have  Bl = B, Bh = B * Q * A^T,  and Bh 
   // is echelonized, containing m nonzero rows  and 24 - m zero rows. 
   // Thus B is the result, with the orthogonal complement of A in 
   // rows m,...,23 of B.  
   //
   // Remark: If the rows of A are linear independent then the upper
   //         k rows of  B * Q * A^T  form a unit matrix.
   m = bitmatrix64_echelon_l(b, 24, 24, k);

   // Output B = Bl and return m
   for (i = 0; i < 24; ++i) b[i] &= 0xffffffUL;
   return m;
}



/**
   @brief Compute invariante spaces for an involution in G_x0 

   Yet to be documented!!!!

   Parameter ``invar`` must be an array of length 12.
*/
// %%EXPORT p
int32_t xsp2co1_involution_invariants(uint64_t *elem, uint64_t *invar)
{
    uint64_t data[40], *pa = data + 16;
    uint64_t x, t0, t1;
    uint_fast32_t i, n;
    int_fast32_t status;

    // Initialize output with zeros and an error bit in row 0
    invar[0] = 0xC000000;
    for (i = 1; i < 12; ++i) invar[i] = 0;

    // Let `pa` be the 24 time 24 unit matrix 1
    for (i = 0; i < 24; ++i) pa[i] = ONE << i;

    // Conjugate row vectors of unit matrix pa with elements
    // and store the the matrix IM of the conjugated
    // row vectors in pa.
    status = xsp2co1_xspecial_conjugate(elem, 24, pa, 0);
    if (status < 0) return status;

    // Next we store two matrices PAH, PAL in pa, with PAL
    // in the lower 32 columns and PAH in the higher 32 columns
    // Put PAH = 1, PAL = IM1, where IM1 = IM + 1 
    for (i = 0; i < 24; ++i) {
         pa[i] &=  0xffffffULL;
         pa[i] ^= 0x100000001ULL  << i;
    }

    // The group element `elem` is an involution (modulo the group  
    // Co_1) iff we have IM1**2 == 0. Otherwise we abort with an error.
    if (square_mat24_nonzero(pa)) return -1; 

    // Echelonize PAL. So we left multiply both, PAH and PAL with a 
    // nonsingular matrix T auch that T * IM1 is echelonized. Then
    // PAH will contain T and PAL will contain  T * IM1.
    // The upper n rows of PAL will contain the image \im IM1
    // The upper n rows of PAH will contain preimages of the rows of PAL
    // The lower 24 - n rows of PAH will contain the kernel \ker IM1
    // The lower 24 - n rows of PAL will be zero
    n = bitmatrix64_echelon_h(pa, 24, 24, 24);

    // Deal with the IM1 == 0, i.e. `elem` is in O_2(G_x0) or,
    // equivalently, `elem` is neutral modulo Co_1.
    if (n == 0) {
        x = xsp2co1_xspecial_vector(pa);
        if (x == 0) {
             *invar = 0;
             return 0;
        }
        x ^= leech_type_mod2(x) << 24;
        x ^= ONE << 25;
        invar[0] = x;
        return 1;
    }
      
    // Deal with a 2A involution in Co_1
    if (n == 8) {
        // Move \ker IM1 to PAL and compute signs for \ker IM1
        bitmatrix64_rot_bits(pa + 8, 16, 32, 64, 0);
        status = xsp2co1_xspecial_conjugate(elem, 16, pa + 8, 1);
        if (status < 0) return status;

        // Make sure that at most the first row of \ker IM1
        // has negative sign
        bitmatrix64_echelon_h(pa+8, 16, 25, 1);

        // Copy the image \im IM1 to output rows t1,...,t1+7, with
        // t1 = 0 if all entries of  \ker IM1 have positive sign
        // and t1 = 1 otherwise.
        t1 = (pa[0] >> 24) & 1;
        for (i = 0; i < 8; ++i) invar[t1 + i] = pa[i];

        // Skip the following steps for n == 8 in case t1 = 0.
        if (t1 == 0) goto final_echelonize;

        // Now the first row vector of the matrix PAL[8..23] 
        // representing \ker IM1 has negative sign and the 
        // other row vectors of that matrix have positive sign.


        // Comupute the orthogonal complement (\im IM1)^+  of the 
        // positive part (\ker IM1)^+ of \ker A. We also compute the
        // orthogonal complement of \ker IM1 (which is \im IM1) in
        // such a way that we can find an vector in (\im IM1)^+
        // that is not in \im IM1. 
        xsp2co1_orthogonal_complement(pa, data, 16);
        // Now we have computed a basis V of the Leech lattice mod 2
        // (in the array ``data``) such that V[16],...V[23] spans
        // the orthogonal complement \im IM1 of \ker IM1. By
        // definition of function ``xsp2co1_orthogonal_complement`` 
        // the vector v0 = V[0] is orthognal to (\ker IM1)^+, 
        // but not to  \ker IM1. 

        // Thus v0 is in  (\im IM1)^+ but not in \im IM1.
        // Copy the v0 to the output 0.
        invar[0] = data[0];

        // Set bit 25 of row i to 1 if i == 0 and to 0 otherwise
        invar[0] |= 0x2000000;

        // Set bit 24 of output row 0 to t0, with t0 = type(v0) (mod 2).
        t0 = leech_type_mod2(invar[0]);
        invar[0] |= t0 << 24;

        // Set bit 24 of the other output rows i = 1,...8 to t[i] with
        // t[i] = t0 + type(v0 + x[i])   (mod 2) .
        // Here x[i] is the output vector in row i.
        for (i = 0; i < 8; ++i) {
            t1 = t0 ^ leech_type_mod2(invar[0] ^ invar[i]);
            invar[i] |= t1 << 24; 
        }

        // Adjust number of output rows to 9.
        n = 9;
    } else if (n == 12) {
        // Copy the image the \im IM1 to the array ``data`` and 
        // store the signs of these images in bit 24.
        for (i = 0; i < 12; ++i) data[i] = pa[i];
        status = xsp2co1_xspecial_conjugate(elem, 16, data, 1);
        if (status < 0) return status;

        // Copy vectors pa[0,...,11] (containing \im IM1 and preimages
        // of these images) to output vector bits 23...0 and 55...32 
        // Copy signs of images to output vector bit 24
        // Copy type(output vector) modulo 2 to output vector bit 25.
        for (i = 0; i < 12; ++ i) {
             invar[i] = (pa[i] & 0xffffff00ffffffULL)
                     | (data[i] & 0x1000000)
                     | (leech_type_mod2(pa[i]) << 25);
        }
    } else return -1;


final_echelonize:
    // Comment this later!!
    bitmatrix64_xch_bits(invar, n, 12, 0x800);
    bitmatrix64_rot_bits(invar, n, 1, 12,0);
    bitmatrix64_echelon_h(invar, n, 26, 26);
    bitmatrix64_rot_bits(invar, n, 11, 12,0);
    bitmatrix64_xch_bits(invar, n, 12, 0x800);
    return n;
    
}






//  %%GEN h
/// @endcond 
//  %%GEN c


// %%GEN ch
#ifdef __cplusplus
}
#endif
